import numpy as np
import pandas as pd
import xarray as xr
from pathlib import Path
import scipy
import subprocess
import logging


def run_fortran_executable(executable_path, nlat, nlev, zn, zb, zrh0, zt0, zu0, zgamma, moisture, filename: Path, logf: Path = None):
    """
    WARNING!!!
    THIS FUNCTION CAN CAUSE THE SHELL TO HANG.
    PREPARE TO MANUALLY CLOSE IT AFTER RUNNING. 
    
    Runs a Fortran executable with the specified arguments.

    Parameters:
        executable_path (str): Path to the Fortran executable.
        nlat (int): Number of latitude ticks.
        nlev (int): Number of levels.
        zn (float): Jet width.
        zb (float): Jet height.
        zrh0 (float): Surface level relative humidity (%).
        zt0 (float): Average surface virtual temperature (K).
        zu0 (float): Affects amplitude of zonal mean wind speed (m/s).
        zgamma (float): Lapse rate (K/m).
        moisture (int): 41 for dry run, 42 for moist.
        filename (Path): Output CSV file path.
    
    Returns:
        stdout (str): Standard output from the executable.
        stderr (str): Standard error from the executable.
    """
    if logf:
        logger = logging.getLogger(__name__)
    args = [
        str(nlat), str(nlev), str(zn), str(zb), str(zrh0), 
        str(zt0), str(zu0), str(zgamma), str(moisture), filename
    ]
     # fortran refuses to write over extant file ... fine.
    if filename.exists(): 
        return None, "CSV already exists, skipping execution."
        
    
    try:
        result = subprocess.run(
            [executable_path] + args, 
            text=True, capture_output=True, check=True
        )
        return result.stdout, result.stderr
    except subprocess.CalledProcessError as e:
        if logf:
            logger.error(f"Error running executable: {e}")
            logger.error(f"Standard Error: {e.stderr}")
            
        else:
            print(f"Error running executable: {e}")
            print(f"Standard Error: {e.stderr}")
        return None, e.stderr


def read_to_df(fort_path: Path, nlat: int) -> tuple[pd.DataFrame, int]:

    # load the data
    mylist = []

    for i, chunk in enumerate(pd.read_csv(fort_path, header=0, index_col=None, chunksize=2000)):
        mylist.append(chunk)

    f_in = pd.concat(mylist, axis= 0)
    del mylist
   

    # make sure given nlat is consistent with the input data
    assert f_in.shape[
        0] % nlat == 0, f"invalid dimensions: nlat x nlat != {f_in.shape[0]} (len(f_in))"
    nlev = f_in.shape[0] // nlat

    # remove leading/trailing whitespace from column names
    f_in.columns = [c.strip() for c in f_in.columns]

    return f_in, nlev


def read_metadata(metadata_dir: Path, lat_fname: str, lon_fname: str, all_lev_fname: str, model_lev_fname: str) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
    # load latitude and vertical level data
    lat = np.load(metadata_dir / lat_fname)
    lon = np.load(metadata_dir / lon_fname)
    all_lev = pd.read_csv(metadata_dir / all_lev_fname,
                      header=0, index_col=None).values
    keep_plevs = pd.read_csv(metadata_dir / model_lev_fname, header=None).values.flatten()

    return lat, lon, all_lev.T, keep_plevs


def compute_tcwv(q: np.ndarray, lev: np.ndarray) -> np.ndarray:
    # compute total column water vapor, see https://resources.eumetrain.org/data/3/359/print_2.htm for similar formula
    g = 9.80665  # m/s^2

    pa = 100 * lev  # convert hPa to Pa

    tcwv = -(1 / g) * scipy.integrate.trapezoid(q, pa, axis=1)

    return tcwv


def process_individual_fort_file(
        csv_path: Path,
        nc_path: Path,
        metadata_dir: Path,
        latitudes_fname: str,
        longitudes_fname: str,
        all_plevels_fname: str,
        model_plevels_fname: str,
        nlat: int,
        write_data: bool = False,
        standardize: bool = False, 
        pl_vars: list[str] = ["R", "U", "V", "T", "Z"],
        sl_vars: list[str] = ["VAR_2T", "VAR_10U", "VAR_10V", "VAR_100U", "VAR_100V", "TCW", "SP", "MSL"],
        means_fname: str | None = None,
        stds_fname: str | None = None,
        logf: Path = None,
):
    """
    Processes a CSV file generated by the Fortran executable into an xarray dataset.
    
    Parameters:
        csv_path (Path): Path to the input CSV file.
        nc_path (Path): Path to save the output NetCDF file.
        metadata_dir (Path): Directory containing metadata files.
        latitudes_fname (str): Filename for latitude data.
        longitudes_fname (str): Filename for longitude data.
        all_plevels_fname (str): Filename for all pressure levels data.
        model_plevels_fname (str): Filename for model pressure levels data.
        nlat (int): Number of latitude ticks.
        write_data (bool): Whether to write the output to disk. Default is False.
        standardize (bool): Whether to standardize the dataset. Default is False.
        pl_vars (list[str]): List of pressure-level variables to include in the dataset.
        sl_vars (list[str]): List of single-level variables to include in the dataset.
        means_fname (str | None): Filename for means data. Required if standardize is True.
        stds_fname (str | None): Filename for standard deviations data. Required if standardize is True.
        logf (Path): Path to log file. Default is None.

    Returns:
        xr.Dataset: Processed xarray dataset.
    """

    df, nlev = read_to_df(csv_path, nlat)

    # extract vertical RH profile, convert to percentage
    R_raw = df["ZRH"].values[:nlev] * 100

    # remove RH from dataframe 
    df = df.drop(columns=["ZRH"])

    # read latitude and vertical levels data
    lat, lon, (plev, etalev), keep_plev = read_metadata(
        metadata_dir, latitudes_fname, longitudes_fname, all_plevels_fname, model_plevels_fname)
    plev, etalev = plev.T, etalev.T
    
    # prepare dict of vars to make checking them easier
    vars = {}
    
    # compute total column water vapor
    vars["Q"] = df["ZQ"].values.reshape(nlat, nlev)
    vars["TCW"] = compute_tcwv(vars["Q"], plev)

    # keep only the desired vertical levels
    keep_idxs = np.where(np.isin(plev, keep_plev))[0]

    # similar to above, tile across lat to match the shape of the other variables
    R = np.tile(R_raw[keep_idxs], (nlat, 1))

    # create pressure vars, both constant due to initiation at sea level
    SP = np.full_like(TCW, 1013.25) * 100  # convert hPa to Pa
    MSL = np.full_like(TCW, 1013.25) * 100  # convert hPa to Pa

    # create xarray dataset
    ds = xr.Dataset(
        {
            "TCW": (["lat"], tcwv),
            "SP": (["lat"], sp),
            "MSL": (["lat"], msl),
        },
        coords={
            "lat": lat, 
            "level": keep_plev
        }
    )

    # determine whether using RH (SFNO) or Q (graphcast)
    if "RH" in pl_vars:
        moisture_var = (("r", "RH"), rh)
        
    elif "Q" in sl_vars:
        moisture_var = ("q", "ZQ")
        
    else:
        print(f"No moisture variable found.")
        
    # iteratively add pressure-coordinate necessary variables and levels to the dataset
    for lname, uname in [("u", "ZU"), ("v", "ZV"), ("t", "ZT"), ("z", "ZPHI_F"), moisture_var]:

        # rh case, processing done already
        if isinstance(lname, tuple):
            sub = uname  # uname contains rh data
            (lname, uname) = lname  # lname contains both names

        # other vars
        else:
            # reshape to (nlat, nlev) and keep only the vertical levels needed for SFNO
            sub = df[uname].values.reshape(nlat, nlev)[:, keep_idxs]

        ds[lname] = xr.DataArray(sub, dims=("lat", "level"))

    # add height level variables to dataset, all at lowest model level [0] or 1013.25 hPa
    u = df["ZU"].values.reshape(nlat, nlev)
    v = df["ZV"].values.reshape(nlat, nlev)
    t = df["ZT"].values.reshape(nlat, nlev)
    # lowest model level instead of 10 meter winds
    ds["u10m"] = (["lat"], u[:, 0])
    ds["v10m"] = (["lat"], v[:, 0])
    # lowest model level instead of 100 meter winds
    ds["u100m"] = (["lat"], u[:, 0])
    ds["v100m"] = (["lat"], v[:, 0])
    # lowest model level instead of 2 meter temperature
    ds["t2m"] = (["lat"], t[:, 0])
    
    if add_w:
        # since the bouvier atmosphere is hydrostatically balanced, w = 0
        sub = np.zeros_like(df["ZQ"].values.reshape(nlat, nlev)[:, keep_idxs])

        ds[lname] = xr.DataArray(sub, dims=("lat", "level"))

    # add dewpoint temperature if requested
    if add_dewpt:
        t2mC = df["ZT"].values.reshape(nlat, nlev)[:, 0] - 273.15
        # calculate dewpoint temperature, formula from https://en.wikipedia.org/wiki/Dew_point
        b = 17.625
        c = 243.04
        gamma = np.log(rh_raw[0] / 100) + (b * t2mC) / (c + t2mC)
        dewpt = (c * gamma) / (b - gamma) + 273.15

        ds["2d"] = (["lat"], dewpt)
        

    # rename to dcmip2025_helper_code standards
        
    ds = ds.rename_vars({
        "t2m": "VAR_2T",
        "u100m": "VAR_100U",
        "v100m": "VAR_100V",
        "u10m": "VAR_10U",
        "v10m": "VAR_10V",
        "tcwv": "TCW",
        "sp": "SP",
        "msl": "MSL",
        "u": "U",
        "v": "V",
        "t": "T",
        "r": "R",
        "z": "Z",
        "q": "Q",
        "w": "W",
    })

    if standardize:
        raise NotImplementedError("Standardization needs to be fixed for 3D structure (lev, lat, lon), currently implemented only for flattened vertical levels")
        # now standardize the dataset
        # means and stds are global and time invariant, unlike other versions of FCN
        means = np.load(metadata_dir / means_fname).reshape(-1, 1)
        stds = np.load(metadata_dir / stds_fname).reshape(-1, 1)

        ds = (ds - means) / stds
        # print out means and stds for each channel for sanity check
        # for i, (ch, m, s) in enumerate(zip(channels, means, stds)):
        #     print(f"{ch}: {m} {s}")

    # expand all variables along longitude dimension
    # while Bouvier et al. only outputs one meridional slice, we need the whole domain for SFNO
    ds = ds.expand_dims({"lon": lon}, axis=1)

    # add time dimension because it's required for the inference function
    ds = ds.expand_dims({"time": [0]}, axis=0)
    
    # flip levels to match SFNO
    ds = ds.sortby("level", ascending=True)
    
    # rename to latitude and longitude to match SFNO
    ds = ds.rename({"lat": "latitude", "lon": "longitude"})

    # save to disk
    if write_data:
        ds.to_netcdf(nc_path)
    
    return ds


if __name__ == "__main__":

    data_dir = Path(
        "/N/slate/jmelms/projects/FCN_dynamical_testing/data/initial_conditions/")

    metadata_dir = Path(
        "/N/u/jmelms/BigRed200/projects/dynamical-tests-FCN/metadata/")

    # process_individual_fort_file(
    #     fort_path=data_dir / "raw_fort_output" / "default.csv",
    #     metadata_dir=metadata_dir,
    #     lat_fname="latitude.npy",
    #     lon_fname="longitude.npy",
    #     lev_fname="p_eta_levels_full.txt",
    #     means_fname="global_means.npy",
    #     stds_fname="global_stds.npy",
    #     standardize=False, 

    #     output_to_dir=data_dir / "processed_ic_sets" / "dcmip2025",
    #     f_out_name="steady_state.h5",

    #     nlat=721,
    #     keep_plevs=[1000, 925, 850, 700, 600, 500,
    #                 400, 300, 250, 200, 150, 100, 50],  # 13 levels used for 73 ch SFNO

    #     include_dewpt=False, # must use 74 ch hens_channel_order.txt for dewpt and q instead of r
        
    #     write_data = False

    # )
   